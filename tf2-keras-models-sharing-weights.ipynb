{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USSV_OlCFKOD"
   },
   "source": [
    "# tf2-keras-models-sharing-weights\n",
    "\n",
    "Based on \"Training a neural network on MNIST with Keras\" : https://www.tensorflow.org/datasets/keras_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TTBSvHcSLBzc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZUMhCXhFXdHQ"
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "haykx2K9XgiI"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A0KjuDf7XiqY"
   },
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XWqxdmS1NLKA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_input = tf.keras.layers.Input(shape=(28, 28))\n",
    "x = tf.keras.layers.Flatten()(model_input)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "model_output_1 = tf.keras.layers.Dense(10)(x)\n",
    "model_output_2 = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "model_1 = tf.keras.Model(model_input, model_output_1, name=\"model_1\")\n",
    "model_1.summary()\n",
    "\n",
    "model_2 = tf.keras.Model(model_input, model_output_2, name=\"model_2\")\n",
    "model_2.summary()\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "model_2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(np.array(model_1.layers[2].get_weights()[0]).flatten(), np.array(model_2.layers[2].get_weights()[0]).flatten()))\n",
    "print(np.array_equal(np.array(model_1.layers[2].get_weights()[1]).flatten(), np.array(model_2.layers[2].get_weights()[1]).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(np.array(model_1.layers[3].get_weights()[0]).flatten(), np.array(model_2.layers[3].get_weights()[0]).flatten()))\n",
    "print(np.array_equal(np.array(model_1.layers[3].get_weights()[1]).flatten(), np.array(model_2.layers[3].get_weights()[1]).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 2.3934 - sparse_categorical_accuracy: 0.1142\n",
      "[2.393404960632324, 0.11423332989215851]\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 2.3934 - sparse_categorical_accuracy: 0.1142\n",
      "[2.3934051990509033, 0.11423332989215851]\n"
     ]
    }
   ],
   "source": [
    "print(model_2.evaluate(ds_train))\n",
    "print(model_2.evaluate(ds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3603 - sparse_categorical_accuracy: 0.8998 - val_loss: 0.1894 - val_sparse_categorical_accuracy: 0.9480\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1610 - sparse_categorical_accuracy: 0.9544 - val_loss: 0.1358 - val_sparse_categorical_accuracy: 0.9606\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.9663 - val_loss: 0.1076 - val_sparse_categorical_accuracy: 0.9692\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0920 - sparse_categorical_accuracy: 0.9734 - val_loss: 0.0937 - val_sparse_categorical_accuracy: 0.9722\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0740 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.0859 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0610 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0791 - val_sparse_categorical_accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f13a004c3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 4.0878 - sparse_categorical_accuracy: 0.0431\n",
      "[4.08781099319458, 0.04309999942779541]\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 4.0878 - sparse_categorical_accuracy: 0.0431\n",
      "[4.087810516357422, 0.04309999942779541]\n"
     ]
    }
   ],
   "source": [
    "print(model_2.evaluate(ds_train))\n",
    "print(model_2.evaluate(ds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(np.array(model_1.layers[2].get_weights()[0]), np.array(model_2.layers[2].get_weights()[0])))\n",
    "print(np.array_equal(np.array(model_1.layers[2].get_weights()[1]), np.array(model_2.layers[2].get_weights()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(np.array(model_1.layers[3].get_weights()[0]).flatten(), np.array(model_2.layers[3].get_weights()[0]).flatten()))\n",
    "print(np.array_equal(np.array(model_1.layers[3].get_weights()[1]).flatten(), np.array(model_2.layers[3].get_weights()[1]).flatten()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow/datasets",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv_tf230",
   "language": "python",
   "name": "venv_tf230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
